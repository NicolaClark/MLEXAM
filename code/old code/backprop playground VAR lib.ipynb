{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-9a80ee59-c5de-4b61-a57b-43c5d666078d",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4c16a707",
    "execution_start": 1638974149644,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "# Copy and pasted from https://github.com/rasmusbergpalm/nanograd/blob/main/nanograd.py\n\nfrom typing import Union\nfrom math import tanh\n\n\nclass Var:\n    \"\"\"\n    A variable which holds a number and enables gradient computations.\n    \"\"\"\n\n    def __init__(self, val: Union[float, int], parents=None):\n        assert type(val) in {float, int}\n        if parents is None:\n            parents = []\n        self.v = val\n        self.parents = parents\n        self.grad = 0.0\n\n    def backprop(self, bp):\n        self.grad += bp\n        for parent, grad in self.parents:\n            parent.backprop(grad * bp)\n\n    def backward(self):\n        self.backprop(1.0)\n\n    def __add__(self: 'Var', other: 'Var') -> 'Var':\n        return Var(self.v + other.v, [(self, 1.0), (other, 1.0)])\n\n    def __mul__(self: 'Var', other: 'Var') -> 'Var':\n        return Var(self.v * other.v, [(self, other.v), (other, self.v)])\n\n    def __pow__(self, power: Union[float, int]) -> 'Var':\n        assert type(power) in {float, int}, \"power must be float or int\"\n        return Var(self.v ** power, [(self, power * self.v ** (power - 1))])\n\n    def __neg__(self: 'Var') -> 'Var':\n        return Var(-1.0) * self\n\n    def __sub__(self: 'Var', other: 'Var') -> 'Var':\n        return self + (-other)\n\n    def __truediv__(self: 'Var', other: 'Var') -> 'Var':\n        return self * other ** -1\n\n    def tanh(self) -> 'Var':\n        return Var(tanh(self.v), [(self, 1 - tanh(self.v) ** 2)])\n\n    def relu(self) -> 'Var':\n        return Var(self.v if self.v > 0.0 else 0.0, [(self, 1.0 if self.v > 0.0 else 0.0)])\n\n    def __repr__(self):\n        return \"Var(v=%.4f, grad=%.4f)\" % (self.v, self.grad)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00001-fad3bf86-7da7-419c-af3e-90b2aa5d0156",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ff143fe",
    "execution_start": 1638974149645,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "import math",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00002-f2d48a49-b7bd-4aaf-886d-748893df2d0e",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6b433fca",
    "execution_start": 1638974149688,
    "execution_millis": 0,
    "deepnote_output_heights": [
     78.765625
    ],
    "deepnote_cell_type": "code"
   },
   "source": "class adiff:\n    \n    def __init__(self, value, parents=None):\n        try:\n            assert type(value) in {float, int}, \"Numbers please\"\n        except AssertionError as msg:\n            print(msg)\n        self.val = value\n        self.gradient = 0.0\n        self.parents = parents\n        if parents == None:\n            self.parents = []\n\n    \"\"\"The backprop, from Rasmusbergpalm's github\"\"\"\n    # def backprop(self, bp):\n    #     self.grad += bp\n    #     for parent, grad in self.parents:\n    #         parent.backprop(grad * bp)\n    #     return self\n    def backprop(self, value):\n        self.gradient+= value\n        for parent, gradient in self.parents:\n            parent.backprop(gradient * value)\n    \n    def backward(self):\n        self.backprop(1.0)\n\n    def __add__(self, o_value):\n        n_value = self.val + o_value.val\n        return adiff(n_value, [(self, 1.0), (o_value, 1.0)])\n\n    def __mul__(self, o_value):\n        n_value = self.val * o_value.val\n        return adiff(n_value, [(self, o_value.val), (o_value, self.val)])\n\n    def __sub__(self, o_value):\n        n_value = adiff(self.val - o_value.val, [(self, 1.0), (o_value, 1.0)])\n        return n_value\n        \n    def __pow__(self, o_value):\n        n_value = self.val**o_value.val\n        return adiff(n_value, [(self, o_value.val*self.val**(o_value.val-1.0)), (o_value, n_value*math.log(self.val))])\n    \n    def __truediv__(self, o_value):\n        n_value = self.val/o_value.val\n        return adiff(n_value,[(self,1.0/o_value.val), (o_value,-self.val/o_value.val**2)])##!!!!!!\n\n    #Returning a f-string, with the value and gradient\n    def __repr__(self):\n        return (f\"adiff value = {self.val}, parents = {len(self.parents)}, gradient = {self.gradient}\")\n\n##divide,relu,sigmoid,softmax \n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00004-6108f5c3-e80d-4d22-9cd6-626a139ce43a",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9d573354",
    "execution_start": 1638974563693,
    "execution_millis": 43,
    "deepnote_output_heights": [
     193.9375
    ],
    "deepnote_cell_type": "code"
   },
   "source": "a = adiff(5)\nj = adiff(2)\ng = a + j\nq = a - j\np = a * j\n#powah = g ** j\ndiv = p / j\nl = p * j\nf = l + j*q\nprint(\"j,\",j)\nprint(\"a,\",a)\nprint(\"g add,\",g)\nprint(\"q minus\",q)\nprint(\"p mult\",p)\n#print(\"power,\",powah)\nprint(\"Division,\",div)\nprint(\"l = p * j\",l)\nprint(\"f = l + 4*q\",f)\nprint(\"\\n\\n backwrd\")\nfor what,i in enumerate([g,q,p,div,l,f]):\n    adiff.backward(i)\n    print(\"\\n\",what,\"\\n\")\n    print(\"j,\",j)\n    print(\"a,\",a)\n    print(\"g add,\",g)\n    print(\"q minus\",q)\n    print(\"p mult\",p)\n    #print(\"power,\",powah)\n    print(\"Division,\",div)\n    print(\"l = p * j\",l)\n    print(\"f = l + 4*q\",f)\n\n\n",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "j, adiff value = 2, parents = 0, gradient = 0.0\na, adiff value = 5, parents = 0, gradient = 0.0\ng add, adiff value = 7, parents = 2, gradient = 0.0\nq minus adiff value = 3, parents = 2, gradient = 0.0\np mult adiff value = 10, parents = 2, gradient = 0.0\nDivision, adiff value = 5.0, parents = 2, gradient = 0.0\nl = p * j adiff value = 20, parents = 2, gradient = 0.0\nf = l + 4*q adiff value = 26, parents = 2, gradient = 0.0\n\n\n backwrd\n\n 0 \n\nj, adiff value = 2, parents = 0, gradient = 1.0\na, adiff value = 5, parents = 0, gradient = 1.0\ng add, adiff value = 7, parents = 2, gradient = 1.0\nq minus adiff value = 3, parents = 2, gradient = 0.0\np mult adiff value = 10, parents = 2, gradient = 0.0\nDivision, adiff value = 5.0, parents = 2, gradient = 0.0\nl = p * j adiff value = 20, parents = 2, gradient = 0.0\nf = l + 4*q adiff value = 26, parents = 2, gradient = 0.0\n\n 1 \n\nj, adiff value = 2, parents = 0, gradient = 2.0\na, adiff value = 5, parents = 0, gradient = 2.0\ng add, adiff value = 7, parents = 2, gradient = 1.0\nq minus adiff value = 3, parents = 2, gradient = 1.0\np mult adiff value = 10, parents = 2, gradient = 0.0\nDivision, adiff value = 5.0, parents = 2, gradient = 0.0\nl = p * j adiff value = 20, parents = 2, gradient = 0.0\nf = l + 4*q adiff value = 26, parents = 2, gradient = 0.0\n\n 2 \n\nj, adiff value = 2, parents = 0, gradient = 7.0\na, adiff value = 5, parents = 0, gradient = 4.0\ng add, adiff value = 7, parents = 2, gradient = 1.0\nq minus adiff value = 3, parents = 2, gradient = 1.0\np mult adiff value = 10, parents = 2, gradient = 1.0\nDivision, adiff value = 5.0, parents = 2, gradient = 0.0\nl = p * j adiff value = 20, parents = 2, gradient = 0.0\nf = l + 4*q adiff value = 26, parents = 2, gradient = 0.0\n\n 3 \n\nj, adiff value = 2, parents = 0, gradient = 7.0\na, adiff value = 5, parents = 0, gradient = 5.0\ng add, adiff value = 7, parents = 2, gradient = 1.0\nq minus adiff value = 3, parents = 2, gradient = 1.0\np mult adiff value = 10, parents = 2, gradient = 1.5\nDivision, adiff value = 5.0, parents = 2, gradient = 1.0\nl = p * j adiff value = 20, parents = 2, gradient = 0.0\nf = l + 4*q adiff value = 26, parents = 2, gradient = 0.0\n\n 4 \n\nj, adiff value = 2, parents = 0, gradient = 27.0\na, adiff value = 5, parents = 0, gradient = 9.0\ng add, adiff value = 7, parents = 2, gradient = 1.0\nq minus adiff value = 3, parents = 2, gradient = 1.0\np mult adiff value = 10, parents = 2, gradient = 3.5\nDivision, adiff value = 5.0, parents = 2, gradient = 1.0\nl = p * j adiff value = 20, parents = 2, gradient = 1.0\nf = l + 4*q adiff value = 26, parents = 2, gradient = 0.0\n\n 5 \n\nj, adiff value = 2, parents = 0, gradient = 52.0\na, adiff value = 5, parents = 0, gradient = 15.0\ng add, adiff value = 7, parents = 2, gradient = 1.0\nq minus adiff value = 3, parents = 2, gradient = 3.0\np mult adiff value = 10, parents = 2, gradient = 5.5\nDivision, adiff value = 5.0, parents = 2, gradient = 1.0\nl = p * j adiff value = 20, parents = 2, gradient = 2.0\nf = l + 4*q adiff value = 26, parents = 2, gradient = 1.0\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00004-c6f389a5-49da-4cf0-b595-9606454bc970",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "8a50c14d",
    "execution_start": 1638973800923,
    "execution_millis": 30,
    "deepnote_output_heights": [
     40.375
    ],
    "deepnote_cell_type": "code"
   },
   "source": "p.parents",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 26,
     "data": {
      "text/plain": "[(adiff value = 5, parents = 0, gradient = 1.0, 2),\n (adiff value = 2, parents = 0, gradient = 1.0, 5)]"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00005-6bc5e9a8-808c-4f65-a65f-3916dd9626da",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "88867723",
    "execution_start": 1638974552101,
    "execution_millis": 4,
    "deepnote_output_heights": [
     328.296875
    ],
    "deepnote_cell_type": "code"
   },
   "source": "###testing on Var\na = Var(5.0)\nj = Var(2.0)\ng = a + j\nq = a - j\np = a * j\n#powah = g ** j\ndiv = p / j\nl = p * j\nf = l + j*q\nprint(\"j,\",j)\nprint(\"a,\",a)\nprint(\"g add,\",g)\nprint(\"q minus\",q)\nprint(\"p mult\",p)\n#print(\"power,\",powah)\nprint(\"Division,\",div)\nprint(\"l = p * j\",l)\nprint(\"f = l + 4*q\",f)\nprint(\"\\n\\n backwrd\")\nfor what,i in enumerate([g,q,p,div,l,f]):\n    Var.backward(i)\n    print(\"\\n\",what,\"\\n\")\n    print(\"j,\",j)\n    print(\"a,\",a)\n    print(\"g add,\",g)\n    print(\"q minus\",q)\n    print(\"p mult\",p)\n    #print(\"power,\",powah)\n    print(\"Division,\",div)\n    print(\"l = p * j\",l)\n    print(\"f = l + 4*q\",f)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "j, Var(v=2.0000, grad=0.0000)\na, Var(v=5.0000, grad=0.0000)\ng add, Var(v=7.0000, grad=0.0000)\nq minus Var(v=3.0000, grad=0.0000)\np mult Var(v=10.0000, grad=0.0000)\nDivision, Var(v=5.0000, grad=0.0000)\nl = p * j Var(v=20.0000, grad=0.0000)\nf = l + 4*q Var(v=26.0000, grad=0.0000)\n\n\n backwrd\n\n 0 \n\nj, Var(v=2.0000, grad=1.0000)\na, Var(v=5.0000, grad=1.0000)\ng add, Var(v=7.0000, grad=1.0000)\nq minus Var(v=3.0000, grad=0.0000)\np mult Var(v=10.0000, grad=0.0000)\nDivision, Var(v=5.0000, grad=0.0000)\nl = p * j Var(v=20.0000, grad=0.0000)\nf = l + 4*q Var(v=26.0000, grad=0.0000)\n\n 1 \n\nj, Var(v=2.0000, grad=0.0000)\na, Var(v=5.0000, grad=2.0000)\ng add, Var(v=7.0000, grad=1.0000)\nq minus Var(v=3.0000, grad=1.0000)\np mult Var(v=10.0000, grad=0.0000)\nDivision, Var(v=5.0000, grad=0.0000)\nl = p * j Var(v=20.0000, grad=0.0000)\nf = l + 4*q Var(v=26.0000, grad=0.0000)\n\n 2 \n\nj, Var(v=2.0000, grad=5.0000)\na, Var(v=5.0000, grad=4.0000)\ng add, Var(v=7.0000, grad=1.0000)\nq minus Var(v=3.0000, grad=1.0000)\np mult Var(v=10.0000, grad=1.0000)\nDivision, Var(v=5.0000, grad=0.0000)\nl = p * j Var(v=20.0000, grad=0.0000)\nf = l + 4*q Var(v=26.0000, grad=0.0000)\n\n 3 \n\nj, Var(v=2.0000, grad=5.0000)\na, Var(v=5.0000, grad=5.0000)\ng add, Var(v=7.0000, grad=1.0000)\nq minus Var(v=3.0000, grad=1.0000)\np mult Var(v=10.0000, grad=1.5000)\nDivision, Var(v=5.0000, grad=1.0000)\nl = p * j Var(v=20.0000, grad=0.0000)\nf = l + 4*q Var(v=26.0000, grad=0.0000)\n\n 4 \n\nj, Var(v=2.0000, grad=25.0000)\na, Var(v=5.0000, grad=9.0000)\ng add, Var(v=7.0000, grad=1.0000)\nq minus Var(v=3.0000, grad=1.0000)\np mult Var(v=10.0000, grad=3.5000)\nDivision, Var(v=5.0000, grad=1.0000)\nl = p * j Var(v=20.0000, grad=1.0000)\nf = l + 4*q Var(v=26.0000, grad=0.0000)\n\n 5 \n\nj, Var(v=2.0000, grad=46.0000)\na, Var(v=5.0000, grad=15.0000)\ng add, Var(v=7.0000, grad=1.0000)\nq minus Var(v=3.0000, grad=3.0000)\np mult Var(v=10.0000, grad=5.5000)\nDivision, Var(v=5.0000, grad=1.0000)\nl = p * j Var(v=20.0000, grad=2.0000)\nf = l + 4*q Var(v=26.0000, grad=1.0000)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=58866f15-1cff-4fef-9525-5c3070562370' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "77432baf-b737-4495-aeb8-fcfe309a12e4",
  "deepnote_execution_queue": []
 }
}